{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e6a58f-1202-42bf-9308-2d1553c5fd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 18:00:32.600858: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c6e19-fce5-46e0-9ee0-b081a44653e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e893b363-e9d6-4910-b4cb-6a6faf354a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcf2d00-4879-4e1c-bc50-d17bf8babc40",
   "metadata": {},
   "source": [
    "<p>- Wenn du mit sehr großen Datensätzen arbeitest, können diese oft nicht vollständig in den Arbeitsspeicher (RAM oder GPU-Speicher) geladen werden. Durch das Verarbeiten von Daten in Batches kannst du kleinere Teile des Datensatzes nacheinander verarbeiten, ohne den gesamten Datensatz auf einmal laden zu müssen. </p>\n",
    "<p>- Bei einer BATCH_SIZE von 16 bedeutet dies, dass 16 Bilder gleichzeitig verarbeitet werden, was viel weniger Speicher benötigt als der gesamte Datensatz. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06771fa-7b39-42a2-acc9-1a66f2b1c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"PlantVillage\",\n",
    "    batch_size = BATCH_SIZE,\n",
    "    image_size = IMAGE_SIZE,\n",
    "    shuffle = True,\n",
    "    seed = 24\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d741b44f-aab8-4e0d-825f-31b3b0515e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276a9a04-36de-4b50-b107-a1971ea4bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = dataset.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962b9d96-d7e1-47e4-9c95-af97a0f52198",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297f25f8-c9ba-4c31-b243-a6cc02343b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, label_batch in dataset.take(1):\n",
    "    print(image_batch.shape)\n",
    "    print(label_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a183b326-e592-4c7a-a82f-9ef9114daf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9302512a-96e2-47d5-9b13-8bbb9f980b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, label_batch in dataset.take(1):\n",
    "    plt.imshow(image_batch[0].numpy().astype(\"uint8\"))\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7933020-8de8-40fa-9ab5-b3232fe16af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9f14a1-865e-4066-bd08-8afd44da680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = [1,29,3,8,3,2,11,56,67]\n",
    "tw.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65db93d1-2374-4d42-bb07-5143487e431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partitioned_datasets(dataset, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(shuffle_size, seed=10)\n",
    "\n",
    "        dataset_len = len(dataset)\n",
    "    \n",
    "        train_len = int(train_split*dataset_len)\n",
    "        train_dataset = dataset.take(train_len)\n",
    "        dataset = dataset.skip(train_len)\n",
    "\n",
    "        val_len = int(val_split*dataset_len)\n",
    "        val_dataset = dataset.take(val_len)\n",
    "        dataset = dataset.skip(val_len)\n",
    "\n",
    "        test_len = dataset_len - train_len - val_len\n",
    "        test_dataset = dataset.take(test_len)\n",
    "\n",
    "        return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803fc42a-8bd6-4e3f-b6bb-942b2982d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = get_partitioned_datasets(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015d682b-a106-4e8f-8cef-c5359a03e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cfd7e2-3100-412b-9b87-0e97c050e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acc8d24-a09d-47d5-a018-a773d4d2bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c668053-6f21-4cd1-af34-b4a3858de08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b55e19a-f2a5-48b6-bbcb-455e86102f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONE_IMAGE_SIZE = 256\n",
    "resize_and_rescale = tf.keras.Sequential([\n",
    "    layers.Resizing(ONE_IMAGE_SIZE, ONE_IMAGE_SIZE),\n",
    "    layers.Rescaling(1.0/255)\n",
    "])\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.2),\n",
    "    layers.RandomTranslation(height_factor=0.2, width_factor=0.2),\n",
    "    layers.RandomContrast(0.2),\n",
    "    layers.RandomBrightness(0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1e3823-d180-47d0-8832-04ee212cce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS = 3\n",
    "input_shape = (BATCH_SIZE, ONE_IMAGE_SIZE, ONE_IMAGE_SIZE, CHANNELS)\n",
    "n_classes = 10\n",
    "\n",
    "model = models.Sequential([\n",
    "    resize_and_rescale,\n",
    "    layers.Conv2D(32, kernel_size = (3,3), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(n_classes, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.build(input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c2e75f-ef02-470a-bbce-ec428206f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b60e105-ec29-4c20-b7e4-4aac06f09abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9492ed-93df-431a-a33a-ca8a4cce1bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=val_dataset,\n",
    "    verbose=1,\n",
    "    epochs=8,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
